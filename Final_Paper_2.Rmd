---
title: "Race, Experiences, and Criminal Sentencing"
author: "Julie Norman, Abraham Alhomadi, and Keaton Manwaring"
date: "12/6/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

# theme_minimal()
# Libraries
library(tidyverse)
library(lmtest)
library(wooldridge)
library(gridExtra)
library(stargazer)
library(corrgram)
library(broom)
library(car)

```

```{r include=FALSE}
#data cleaning 
df <- filter(crime1, avgsen > 0)

df$race <- with(df, ifelse(df$black == 1, "black", 
                           ifelse(df$hispan == 1, "hisp", "non")))

df$fnfarr86 <- as.factor(df$nfarr86)
df$fnarr86 <- as.factor(df$narr86)

```


In this paper, we will undertake an econometric examination using the Crime1 dataset in the Wooldrige Package. The dataset contain a total of 2725 observations on 16 distinct variables.It was collected in 1986, in the State of California. Looking at the data, we will aspire to answer the following research question that is the focus of this paper: "How does an individual's income affect average sentencing outcome when they commit a crime? This question has been the subject of policy research for decades and, using the Crime1 dataset, we would like to contribute to the conversation on this issue of economics and criminal justice reform. Our belief as citizens is that we have a collective responsibility to promote better policies and the well-being of our nation through the efficient and just allocation of resources and ensuring that our criminal justice system is in fact "just" in its treatment of citizens who are convicted of a crime. In addition to our focus research question, we will investigate other equally critical questions, some of which include,"Does a person's employment status increase or decrease average sentencing?", " Do blacks get different average sentence lengths than nonblacks?",  and "how does an individual's prior criminal history, if any, contribute to their average sentencing length?". 

Now, to carry out our investigation, we will begin by focusing our annalysis on the the `r nrow(filter(df, avgsen > 0))` individuals in our dataset who have been sentenced. We're filtering out the individuals who have not served prison time as our question is focused on what factors impact sentence length, not whether they've been found guilty at all. We'll regress our dependent variable "avgsen", which is the average sentence length of a convicted criminal, on variant explanatory variables: inc86 (legal income), nfarr86 (felony arrests), pcnv (proportion of prior convictions), durat (recent unemp duration), narr86(times arrested), Black, and Hispanic. Since our data treats black and hispanic as mutually exclusive, we're going to combine our race variable into a single dummy that illustrates whether an individual is black, hispanic, or neither. Our multiregression model would start with the following model and build on from there to test for interaction between variables and to reflect for possible increase or decrease in sentencing length in the short- and long-term. This is our original regression model:

$$ Avgsen= \beta_{0} + \beta_{1}inc86 + \beta_{2}nfarr86 + \beta_{3}narr86 + \beta_{4}pcnv + \beta_{5}durat + \delta_{1}race + u $$


The additional questions will help put our focus question in perspective, and allow us to identify alternative explanations besides an individual's income to why average sentencing length would differ from one person to another, given variables like employment status, criminal history, and race. 


Lastly, for so long, policy makers have been trying to answer the above questions in order to improve economic and criminal justice policy making. They endeavor to identify our criminal justice system flaws and seek to address them, including issues like sentencing discrimination on the basis of income and race, and to improve the efficient and fair allocation of economic resources to all their citizens. This is an interesting economic inquiry, for it touches the heart of one of the challenges that has engulfed our nation. A stable and functioning economic system and a fair criminal justice system are important to maintaining a healthy life style, public safety to guard against social division because of discriminatory economic and criminal justice motives, and to promote a flourishing, economically strong country in the 21st century. For this reason, we are seeking to investigate whether average sentencing in, say, California or other states for that matter, are rendered based on effective, just, and fair laws, or are there discriminatory motives that impact our justice system rendering of sentencing. If discrimination does exist, then as citizens, we have a solemn obligation to put forth facts backed up by objective data and partake in making our country, as our founding fathers envisioned, a "more perfect union." 

Our Crime1 dataset is included in the wooldridge package. We did not collect the data on our own nor do we know precisely how the data was collected. However, we know that the data it contains was collected in the State of California and the arrests data are for the year 1986. The observations reflect men who were born in 1960 or 1961, and the subject men in the dataset all had criminal history and a minimum of one arrest before 1986. So, our regression model, as written above, will provide a multidimensional analysis to explain whether our independent variables listed do or do not have a causal relationship with our dependent variable, Avgsen. We wanted to include gender as a dummy variable in our regression model, but the dataset only contains data on men. Therefore, instead of looking at the relationship between gender and average sentencing, we will dwell on race and average sentencing. 

## Data Exploration

In order to better understand our data we'll look at each variable to check for any violations of the first Gauss Markov assumption: linear parameters. While there's logical expanations for relationships between our explanatory variable and our response variables, we also need to ensure that our data bares out that relationship.

### Average Sentence Length

```{r echo=FALSE}
ggplot(df, aes(x = avgsen))+ 
  xlab("Length in Months") + ylab("Count") + 
  ggtitle("Average Sentence Length Histogram") +
  geom_histogram(binwidth = 5) + 
  theme_minimal()
```
The histogram shows that that the distribution of sentence length is bell shape with a slight right skew indicating that a log might be an appropriate adjustment for our model. However, since there are zero values in the variable a log transformation isn't appropriate. 


### Income from 1986

```{r echo=FALSE}
ggplot(df, aes(x = inc86))+ 
  xlab("Income") + ylab("Count") + 
  ggtitle("Histogram of 1986 Legal Income") +
  geom_histogram(binwidth = 20) + 
  theme_minimal()
```

The histogram shows a strong right hand skew indicating that in order to meet the linear parameters assumption, we will need to take the log of the variable. However, since there are so many 0 values we won't be transforming the data Next we need to visualize the relationship between the income and average sentence length. 

```{r echo=FALSE}
ggplot(df, aes(x = inc86, y = (avgsen))) + 
  xlab("Income") + ylab("Average Sentence Length") + 
  ggtitle("1986 Legal Income and Average Sentence Length") +
  geom_smooth(method = 'lm') +
  geom_point() + 
  theme_minimal()
```

The model indicates a week negative association between the income variable and the sentence variable. Due to this, we shouldn't expect a strong impact of this variable in our model. 

### Felony arresting 

```{r echo=FALSE}
ggplot(df, aes(x = nfarr86)) + 
  xlab("1986 Felony Arrests") + ylab("Count") + 
  ggtitle("Historgram of 1986 Felony Arrests") +
  geom_histogram(binwidth = 1) + 
  theme_minimal()
```

The histogram indicates a strong right hand skew in felony arrests. However, the ordinal nature of the data makes it unideal for a multiple regression model. 


```{r echo=FALSE}
ggplot(df, aes(x = fnfarr86, y = avgsen)) + 
  xlab("1986 Felony Arrests") + ylab("Average Sentence in Months") + 
  ggtitle("1986 Legal Income and Average Sentence Length") +
  geom_boxplot() + 
  theme_minimal()


```

The side by side boxplots illustrates that amoung the average sentence length for each number of felony arrests has essentially no association on average. Due to this and the ordinal nature of the variable we should consider removing this variable from our model.  

### Number of Arrests in 1986

```{r echo=FALSE}
ggplot(df, aes(x = narr86)) + 
  xlab("Arrests") + ylab("Count") + 
  ggtitle("Historgram of 1986 Arrests") +
  geom_histogram(binwidth = 1) + 
  theme_minimal()
```

The histogram indicates a strong right hand skew in arrests. However, the ordinal nature of the data makes it again unideal for a multiple regression model. 


```{r echo=FALSE}
ggplot(df, aes(x = fnarr86, y = avgsen)) + 
  xlab("1986 Felony Arrests") + ylab("Average Sentence in Months") + 
  ggtitle("1986 Legal Income and Average Sentence Length") +
  geom_boxplot() + 
  theme_minimal()

```

Again, he side by side boxplots indicates that amoung the average sentence length for each number of arrests there is essentially no association. While the average sentence for those with 4 felony arrests is higher than the rest, this is merely one data point and as it does not imply an overall trend in the data it's unlikely to be influencial in our model. Due to this and the ordinal nature of the variable we should consider removing this variable from our model.

### Proportion of prior convictions 

```{r echo=FALSE}
ggplot(df, aes(x = pcnv)) + 
  xlab("Proportion of Prior Convictions") + ylab("Count") + 
  ggtitle("Prop of Prior Convictions") +
  geom_histogram(binwidth = .07) + 
  theme_minimal()
```

The histogram indicates that the distribution of th proportion of prior convictions is approximately normal, indicating that this variable is a good candidate for use in a multiple regression model. 

```{r echo=FALSE}
ggplot(df, aes(x = pcnv, y = avgsen)) + 
  xlab("Proportion of Prior Convictions and Average Sentence Length") + ylab("Sentence Length in Months") + 
  ggtitle("Prop of Prior Convictions") +
  geom_smooth(method = 'lm') +
  geom_point() + 
  theme_minimal()
```

This scatterplot indicates a negative association between prior convictions and sentence length. This indicates that as convictions become more common for an individual, their sentence length decreases. This trend is particularly interesting as it does fit a typically assumption of repeat offenders having harsher punishments. While we can only speculate as to the exact nature of this relationship, its possible that other variables such as the types of crimes committed and/or race have a larger impact on average sentence then we wouldn've prevously thought. That makes this variable an ideal candidate for a multiple regression since we can see how this variable combined with other indicators appropriate predict average sentence length.

### Duration of Unemployment

```{r echo=FALSE}
ggplot(df, aes(x = (durat))) + 
  xlab("Duration in Months") + ylab("Count") + 
  ggtitle("Histogram of Unemployment Duration") +
  geom_histogram(binwidth = 5) + 
  theme_minimal()
```

The histogram indicates a strong left hand skew in unemployment duration. This indicates that a logarithmic model might be more appropriate for this variable, however since we have zero values in our data a log isn't appropriate. The measurement unit for duration of unemployment is not given in our data set but since all the other time variables are measured in months we're assuming that here as well. 

```{r echo=FALSE}
ggplot(df, aes(x = (durat), y = (avgsen))) + 
  xlab("Unemployment Duration in Months") + ylab("Average Sentence in Months") + 
  ggtitle("Unemployment and Sentence Length Scatterplot") +
  geom_smooth(method = 'lm') +
  geom_point() + 
  theme_minimal()
```

This scatterplot illustrates a weak positive association between unemployment length and average sentence length. Due too the weakness of the association 

### Race Comparisons


```{r echo=FALSE}
ggplot(df, aes(x = race, y = avgsen)) + 
  xlab("Race") + ylab("Average Sentence in Months") + 
  ggtitle("1986 Legal Income and Average Sentence Length") +
  geom_boxplot() + 
  theme_minimal()

```

The boxplots of average sentence length show minmimal variation accross different racial categories. This indicates that even if there is a statistical significance between the regressions, it is likely of little practical significance. However, since race is a major point in our research question we'll keep this variable in our regression.  

## Model Specification 

Since we're considering eliminating two of the variables, it's important we include them in our models until we've determined the best form for the other variables in our model. 

During our exploration of the variables, we found that due to skewness, a few of our variables could benefit from logs. However, since there are zero values in all of our variables this isn't an option. However, just to be safe we'll compare a variatey of quadratic functions on the variables that weren't as skewed to see if it improves our model. 

```{r echo=FALSE}

reg1 <- lm(avgsen ~ (inc86) + narr86 + nfarr86 + pcnv + durat + race, df)
reg2A <- lm(I(avgsen + avgsen^2) ~ inc86 + narr86 + nfarr86 + pcnv + durat + race, df)
reg2B <- lm((avgsen) ~ inc86 + narr86 + nfarr86 + I(pcnv + pcnv^2) + durat + race, df)
reg2C <- lm(I(avgsen + avgsen^2) ~ inc86 + narr86 + nfarr86 + I(pcnv + pcnv^2) + durat + race, df)

stargazer(reg1, reg2A, reg2B, reg2C,  type = "text")
```


The $R^2$ strongly indicate that our non quadratic model is the best model to work on.  

Next we'll compare models with and without narr86 and nfarr86 to see if their impact justifies keeping them in the model. 

```{r echo=FALSE}
reg3 <- lm((avgsen) ~ (inc86) + pcnv + durat + race, df)
stargazer(reg1, reg3, type = "text")
```

Including narr86 and nfarr86 not only increases our adjusted $R^2$ by 3 percentage points but they are also both statistically significant at the 90% confidence level so we'll keep them in our model. As there is no indication in our scatterplots for a quadratic form, which we'll confirm later, for now we can consider the following regression our current final model

$$ \widehat{log(Avgsen)} = 2.559 - 0.001(inc) + 0.282(narr86) - 0.428(nfarr86) - 0.488(pcnv) + 0.034(durat) - 0.234(racehisp) - 0.353(racenon) $$

## Model Assumptions 
                                      
We used an Ordinary Least Square (OLS) estimating method to compute the coefficients of our parameters. Our model complies with all five of OLS multiple regression assumptions. First, our regression model is linear in parameters, as illustrated by our model above. Additionally, the sampling of the observations was undertaken randomly so we've met that condition as well. The third condition is that there's no multicollinearity. Since the independent variables is `r var(df$avgsen)` which is greater than zero and we have no variable repetition so we just need to check that we have no variables that are perfect multiples of each other 

```{r echo=FALSE}
inc86_lm <- lm((inc86) ~ nfarr86 + narr86 + pcnv + durat + race, df)
nfarr86_lm <- lm(nfarr86 ~ (inc86) + narr86 + pcnv + durat + race, df)
narr86_lm <- lm(narr86 ~ (inc86) + nfarr86 + pcnv + durat + race, df)
pcnv_lm <- lm(pcnv ~ (inc86) + nfarr86 + narr86 + durat + race, df)
durat_lm <- lm (durat ~ (inc86) + nfarr86 + narr86 + pcnv + race, df)
black_lm <- lm(black ~ inc86 + nfarr86 + narr86 + durat + hispan, df)
hispan_lm <- lm(hispan ~ inc86 + nfarr86 + narr86 + durat + black, df)

# vif(inc86_lm)
# vif(nfarr86_lm)
# vif(narr86_lm)
# vif(pcnv_lm)
# vif(durat_lm)
# vif(black_lm)
# vif(hispan_lm)

vif_list <- list(vif(inc86_lm), vif(nfarr86_lm), vif(narr86_lm), vif(pcnv_lm), vif(durat_lm), vif(black_lm), vif(hispan_lm) )
names(vif_list) <- c("Regress on inc86", "Regress on nfarr86", "Regress on narr86", "Regress on pcnv", "Regress on durat", "Regress on Black", "Regress on Hispanic")
vif_list
```

The highest variance inflation factor of all the parameter regressions is 3.555754 while most are between one and two. Values between 1 and 5 are considered moderately correlated while values greater than 5 near perfect collinearity. Since our strongest correlation is a moderate correlation we've shown there's no multicollinearity and we've met this assumption. The next assumption is the zero conditional mean assumption ie $E(u | inc, narr86, nfarr86, pcnv, durat, race) = 0$. We've already added logs to the model where appropriate and since the visualizations of our variables don't indicate a need for quadratics in our model, we can double check our model specificatioon by looking at the model residuals. 


```{r echo=FALSE}
reg1_df <- augment(reg1)
ggplot(reg1_df, aes(x = .fitted, y = .resid)) + 
  ggtitle("Residuals Plot for Regression") +
  xlab("Fitted Values") + ylab("Residuals") + 
  geom_hline(yintercept = 0, colour = "blue") +
  geom_point() + 
  theme_minimal()
```

Since the residuals are more clustered below. The final assumption is homoskedasticity, in otherwords that $Var(u|inc, narr86, nfarr86, pcnv, durat, race) = \sigma^2$. Our visualizations don't indicate any clear signs of homoskedasticity but to verify this we'll conduct a breusch-pagan test and a white test. The hypothesis test is as follows: $ H_{0}: Var(u|inc, narr86, nfarr86, pcnv, durat, race) = Var(u|X) = \sigma^2$ and the alternative where these values are not equal.
                
First we will use the BP test using the F statistic, then White test using F statistic, and both tests using LM ratio. Then, we will discuss the difference in the results they render. Keep in mind that a large F statistic or a large Lagrange multiplier statistic, (LM) indicate a rejection of the null hypothesis. 

```{r echo=FALSE}
# BP test (using F statistics) manually
#summary(lm(reg2A_df$.resid ~ log(inc86) + narr86 + nfarr86 + pcnv + durat + race, df))

bptest(reg1)
```

Using the BP test, we see that our p-value is relatively large (p-value = 0.197) meaning that our F-statistic is relatively small. Consequently, we fail to reject the null hypothesis. This means our unobserved distribution is heteroskedastic. Since we failed the bp test we'll also do a white test, it is a less vigorious test for heteroskedasticity. 

```{r echo = FALSE}
# White test (using F statistic)
u_hat <- reg1$residuals
y_hat <- predict(reg1)

summary(lm(u_hat^2 ~ y_hat + I(y_hat^2)  ))
```
We have an F statistic of 2.016 and a p value of 0.1373 >.05. Therefore we fail to reject the null hypothesis indicating we have an issue with heteroskedasticity. Due to the results of the bp and white test we will use robust standard errors to correct for the heteroskedasticity. 

Below, we compare our previous regression with a new regression that includes robust standard errors. .

```{r echo=FALSE}
range(y_hat)
h_hat <- y_hat *(1-y_hat)
w   <- 1/(h_hat^2)
reg1_wls <- lm((avgsen) ~ (inc86) + narr86 + nfarr86 + pcnv + durat + race, weights = w, df)

stargazer(reg1, reg1_wls, type = "text")

```

Not only is there an increase in the adjusted $R^2$ by .025 points, we also have three additional statistically significant variables: narr86, nfarr86, racenon. Due to this output and the results of the robustness tests, we'll utalise the equation with the robust standard errors. In that equation there are three numeric equations that are not statistically significant. In order to determine whether or not they should be in our model we need to determine whether or not they are jointly significant with the following hypothesis test with an F distribution. 

$$H_{0}: inc86 = 0, pcnv = 0, durat = 0$$
$$H_{1}: inc86 \neq 0, pcnv \neq 0, durat \neq 0 $$

```{r echo=FALSE}
reg4_wls <- lm((avgsen) ~  narr86 + nfarr86 + race, weights = w, df)

Ho_joint <- c("inc86 = 0", "pcnv = 0", "durat = 0")
linearHypothesis(reg1, Ho_joint)
```

The results of the hyothesis test has a low F value of 2.1274 with ap value of 0.1 which indicates that these values are not jointly significant so we'll remove those variables from our model, we can see the comparison with the removed variables here

```{r}
stargazer(reg1_wls, reg4_wls, type = "text")
```
 
By removing the variables we've increased the adjusted $R^2$ and our F statistic indicating that this is the appropriate approach. So our final model is 

$$ \widehat{ log(avgsen)} = 13.969  + 4.838(narr86) - 6.085(nfarr86) - 2.78(racehisp) - 2.9(racenon) $$


## Model Interpretation and Implication

For every additional arrest an individual has, are model suggests an increase in average sentence length by 4.8 months. This result is statistically significant at the 99% confidence level but more importantly it has practical significantin that it supports the understanding that those under frequent suspicion tend to experience more prison time even if those arrests don't lead to jailtime. The next variable has an unexpected association. Every additional felony arrest, our model suggests a 6.1 month decrease in average sentence length. This result is also statistically significant at the 99% confidence level. This result could stem from felonies having harsher punishments which encourages individuals to fight those charges harder in order to have a shorter sentence. Our model also includes dummy variables for race where individuals are considered hispanic, black, or neither. Relative to black individuals, being hispanic decreases average sentence length by 2.8 months and being in neither category decreases average sentence length 2.9 relative to black individuals. Suffice to say that black indivuals tend to have the longest sentences, but its particularly interesting that hispanic and non black/non hispanic individuals are only seperated by about three days difference in average sentencing. This indicates that that race plays a role in how individuals experience the criminal justice system, particularly for those of african heritage. However, the adjusted $R^2 = 0.06$ meaning that our variables can explain only 6% of the variation in average sentence length. So we know that despite our model being statistically significant at the 95% confidence lever, it is far from illustrating the entire story of an individual's average sentence length. 

## Conlcusion 



